---
cid: 20214
node: [NDVI LED Simulator](../notes/MaggPi/07-24-2018/ndvi-led-simulator)
nid: 16777
created_at: 2018-07-24 20:53:26 +0000
timestamp: 1532465606
uid: 554
author: [cfastie](../profile/cfastie)
---

MaggPi,

This a really promising approach. 

Do you have any information on the proportion of red light to NIR light produced by your LED combinations? Ideally you would want to know the radiance in each band (how much energy is being delivered to the test surfaces). That will require special equipment, so you can substitute a measure of brightness. The goal would be to adjust the brightness of each color band (red and NIR) to match the proportion in sunlight (or sky light).

Do you have any information on the relative sensitivity of the camera sensor to red light versus NIR light? Even if you adjust the incoming light to match the red:NIR ratio in sunlight, if the camera is less sensitive to NIR (which it is) then your photographs might not record what you need them to record, which is how much of each color band (red and NIR) is being reflected from the test surfaces.

To review, traditional NDVI images are produced under sunlight with radiometers which measure how much energy is in the light (in red and NIR bands) reflected from foliage. 

Answering the two questions above is a challenge. If you don’t know how sensitive your camera is to red versus NIR light, then you can’t use that camera to measure the relative brightness of red and NIR in the incoming light (or the reflected light). 

A solution which has been proposed is to acquire objects with surfaces of known reflectivity in the red and NIR bands of interest. Ideally each of these surfaces would reflect the same proportion of the red and NIR bands because this will make your calculations easier. But as long as you know how reflective the surface is in both bands you can work with it. By including these surfaces in your photos you can begin to learn how much your lighting and measurement systems are altering the red:NIR ratio.

Unfortunately, you have two unknown variables. Both your incoming light (LEDs) and your camera sensitivity are unknown. So you will have to devise some clever protocol for solving for two unknowns. 

Calibration targets of known reflectivity are commonly used for NDVI photography done outside where sunlight is illuminating the scene and is not an unknown. Ideally we want a camera which will record the actual brightness for both red and NIR reflected from that calibration target. The cameras we use typically underreport NIR because the cameras are less sensitive to NIR. Because we know the expected red:NIR ratio after reflection from the target, we can use the difference between actual reflected red and NIR to adjust the red:NIR ratio in all other pixels in the photo.

Ned Horning’s Photo Monitoring plugin for Fiji allows information from calibration targets to be used to adjust (calibrate) the photos before they are used to compute NDVI. I tried to explain how this works in this note (https://publiclab.org/notes/cfastie/05-01-2016/calibration-cogitation) which assumes that the photos were taken outside under sunlight.

A next step might be to characterize some of the objects in your test photos so you know how much of the incoming red and NIR are being reflected from them. You might already have some information about this for some of the objects you included (e.g., Rosco filters). But this will not solve the two unknown issue. The tool of choice to characterize the spectral reflectance of a surface is a spectrometer. The Public Lab type spectrometers are probably not sufficiently sensitive in the NIR to work well for this, and are also not well-suited for intensity measurements without the type of gain correction that Dave Stoft has done (https://publiclab.org/notes/stoft/02-25-2015/plab-spectrometer-gain-correction).

You have not explained what your two figures represent. Each figure has seven images, and they apparently represent stages in the image sequencer process. It might help to have a caption which explained that there are two input photographs (left) and two images which result from processing (center), and then three images which result from other processing of the NDVI image (right). Some of the things I do not know about these figures are 

1 What is red blend?  
2 Why are there no arrows connecting the two input photos and the NDVI image?  
3 What color channels (RGB) were used to make an NDVI image from the two photos?  
4 What is the default colormap?  
5 What is the stretched colormap?  

There are some things I do know about these figures that many other readers might not know including 

1) What is the fastie colormap?  
2) How is NDVI derived from the input photos?  

My interpretation of the very high (close to 1.0) NDVI values in the red-NIR result is that the red photo was very dark (underexposed?) so the DNs in the red (and also green and blue) channels for the leaves and ND filters were close to zero. If the value for red is zero, then (NIR-red)/(NIR+red) evaluates to 1.0 regardless of the NIR value. The photos (or channels) used for both NIR and red must be well exposed but both must also be exposed with the same settings for f/stop, shutter speed, ISO, and gain.

Chris



[cfastie](../profile/cfastie) replying to: [NDVI LED Simulator](../notes/MaggPi/07-24-2018/ndvi-led-simulator)


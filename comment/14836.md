---
node: Using a Rhodamine B standard to correct some past research mistakes...
author: stoft
created_at: 2016-06-30 19:01:53 +0000
timestamp: 1467313313
nid: 13248
cid: 14836
uid: 54025
---



[stoft](../profile/stoft) replying to: [Using a Rhodamine B standard to correct some past research mistakes...](../notes/dhaffnersr/06-29-2016/using-a-rhodamine-b-standard-to-correct-some-past-research-mistakes)

----
Hmmm ... all of these signals are so close to each other, I suspect that separating them is not so trivial a task ..... while that application is capable of performing the math, it is not clear to me that the resulting plot necessarily represents what it appears to show. The potential trouble concerns removal of signal information (and, therefore, the extraction of other signal information) when signals have been combined -- as they are in the original spectrum. It is not always possible to separate signals this way without, among other errors, the resulting signal containing large uncertainties.

Admittedly, it does depend on the nature of each of the signals. So, to avoid letting such errors "creep in" and create illusions in the resulting data, it is generally best to plot the full, unprocessed original measurement data first. Then, performing single processing steps (and plotting each step's data on the same graph as the original) forces each manipulation of the original data to be shown -- which also forced the requirement of discussing why the result is, or is not, valid and what impact that processing has on the data: e.g. why is averaging valid?, what information is thrown away before the next step? how has the measurement error been affected?, etc..... Without this, the reader (and maybe the author as well) is just taking the giant, hidden, set of processing steps on faith -- a process which is very prone to critical "oops!".

Averaging has it's value -- but it must be declared as part of the processing and that effect must be documented (eg. plotted) since it throws away information. If the goal is only to look for very general "shapes" to spectra, averaging can filter out that information. However, the type of filter can be important as filters can actually create artifacts that look like real data. It's a similar protocol of maintaining the trail of evidence so the source is never lost or contaminated in a way which either obscures or destroys information.

Numerically, 1.6nm is really good. However, that is just a theoretical value based on true colimated light and it assumes infinite pixel resolution. Step 2, is to look carefully at some data from the spectrometer in question and find the wavelength and pixel number range --i.e. calc the ratio of (nm span) / (# pixels) which is the detector's spatial resolution of the spectra. e.g. [ (730nm - 420nm) / (pixel#650 - pixel#320) ] x 3 = [(730-420)/(650-320)]x3 = (310/330)x3 = 0.94 x 3 = 2.8nm CCD detector bandwidth limit.

The third significant bandwidth limiting factor is the colimation error -- that light "rays", which are not parallel, can still illuminate the camera but they will be at a different incident angle to the ideal 'colimated' "rays" -- so their diffraction angles will be slightly different from ideal -- which causes "smearing" of the spectral image at the CCD sensor -- which adds to lower the resolution -- but is harder to estimate.

Resolution limits generally cannot be eliminated by simple post-processing. Averaging of multiple measurements can help some by reducing gaussian noise, but that is not really attacking the fundamental resolution limits. Fortunately, many signals of interest are relatively broad compared with the resolution. It is signals like those of a CFL or laser whose accurate measurement are most affected by the resolution limits.

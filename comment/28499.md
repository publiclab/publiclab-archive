---
cid: 28499
node: ![GSoC 2021 Proposal: Geographic Features Refinement](../notes/barun1024/03-24-2021/gsoc-2021-proposal-geographic-features-refinement)
nid: 26021
created_at: 2021-04-10 12:08:14 +0000
timestamp: 1618056494
uid: 647185
author: barun1024
---

>Are there even websites out there that can be set up to scrape for us? 
Or, would it be possible to write the scraping script, but to have it dump into an existing geoJSON service?

One way would be to schedule cron jobs for scraping and dumping data and leverage a microservice to serve that data. We can use `github-action` to schedule this too. [Ref](https://docs.github.com/en/actions/reference/events-that-trigger-workflows#scheduled-events) 

I am not exactly sure what do we want to `scrape`, since we already have an `api` for people and posts, hence I used the term `cache` to as to reduce the load on the `api`. Do we plan to use some external website which we would need to scrape for data?
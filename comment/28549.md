---
node: GSoC 2021 Proposal: Geographic Features Refinement
author: warren
created_at: 2021-04-13 15:22:12 +0000
timestamp: 1618327332
nid: 26021
cid: 28549
uid: 1
---



[warren](../profile/warren) replying to: [GSoC 2021 Proposal: Geographic Features Refinement](../notes/barun1024/03-24-2021/gsoc-2021-proposal-geographic-features-refinement)

----
I think there are a few options on this. But the main thing is that we are making arbitrary bbox (bounding box) queries to an API using 2 latitudes and 2 longitudes, so a URL-based caching system probably won't work well, since people will zoom and drag the map in unpredictable ways. 

However, the results are JSON, so we might think about how to store JSON objects in a geographic index for fast retrieval. Our database is actually not set up for that, as we use the more general purpose tags "lat:____" and "lon:____" and so we can't do as efficient database indexing on those.

https://publiclab.org/api/srch/taglocations?nwlat=47.15984001304432&selat=34.19817309627726&nwlng=-80.79345703125001&selng=-61.19384765625001

this is the query we get when we go to https://publiclab.org/map#6/41/-71, for example. Learn about this kind of API query here: https://publiclab.org/wiki/api

So I think we can think about 2 components - the "storage/retrieval" using a query /like/ the one above, and the mechanism to "scrape and store" from the PL API into that storage system. 

For storage/retrieval, there seem to be many good options, i'm thinking node.js for lightweight and fast, but we can be flexible: https://duckduckgo.com/?q=efficient+nodejs+geojson+database&atb=v121-6&ia=web

For the scraping, I'm not sure. Maybe we could think about whether we are constantly scanning by region, or if accessing the microservice triggers a deferred re-fetch from the original API? These are really good design/architecture questions to weigh pros/cons on. 

Thanks [@barun1024](/profile/barun1024)!!!
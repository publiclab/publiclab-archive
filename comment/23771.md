---
cid: 23771
node: [GSoC proposal: Mapknitter ORB Descriptor (w/ auto-stitching, pattern training, and live video support) and LDI revamp (major UI enhancements)](../notes/rexagod/03-11-2019/gsoc-proposal-mapknitter-orb-descriptor-w-auto-stitching-pattern-training-and-live-video-support-and-ldi-revamp-major-ui-enhancements)
nid: 18515
created_at: 2019-04-08 22:59:40 +0000
timestamp: 1554764380
uid: 564358
author: [rexagod](../profile/rexagod)
---

Hello [@warren](/profile/warren)! Here's my response regarding the code section in your comment above. I've implemented these keeping the image space in mind, rather than the canvas one.

![Location](https://user-images.githubusercontent.com/33557095/55765697-913c1300-5a8e-11e9-9b60-df690b0b9538.jpg)

```js
function ORBMatcher() {
  this.findPoints = findPoints;
  this.findMatchedPoints = findMatchedPoints;
  this.projectPointsInto = projectPointsInto;
  this.renderer = renderer;
  return this;
}

function findPoints(img) { //extracts [RGBA] info about every pixel in an image passed to it
  //img is an `Image()` instance
	var canvas = document.createElement("CANVAS");  //notice we aren't appending this anywhere (offscreen)
	//TC is a temporary canvas used to extract [RGBA] data for each pixel of image
	var TC = node.getContext("2d");
	TC.drawImage(img,0,0,img.width,img.height); //faster than putImageData
	canvas.parentElement.childElement(canvas); //remove canvas since we are done with canvas APIs
	//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	//imgData contains `0-255` "clamped" points in a Uint8ClampedArray (refer MDN)
	//for eg., if the image is of the resolution 640x480, this array will have 1228800
	//elements (307200x4), where all are "clamped" (rounded off) to have a value
	//between 0-255, and every set of 4 elements represents the R,G,B, and A values
	//for every single pixel, i.e., 307200 pixels
	var imgData = TC.getImageData(0, 0, primary_image.width, primary_image.height);
	//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	//finally, we can extract the data about every single pixel, which also removes
	//the need for having a `Stateful` module (saving space as well) since we
	//now already have everything we need
}

// construct correspondences for inliers
// var correspondences_obj;
// for (var i = 0; i < count; ++i) {
//  var m = matches[i]; //`match_t()` construct:176
//  var s_kp = screen_corners[m.screen_idx]; //`keypoint_t()` construct:175
//  var p_kp = pattern_corners[m.pattern_lev][m.pattern_idx];
//  pattern_xy[i] = { "x": p_kp.x, "y": p_kp.y }; //==>X (img1points)
//  screen_xy[i] = { "x": s_kp.x, "y": s_kp.y };  //==>Y (img2points)
// }
// correspondences_obj = {"X":pattern_xy,"Y":screen_xy};
function findMatchedPoints(X,Y) {
	var good_matches_locatorX=[];
	var good_matches_locatorY=[];
	var strong_inliers=0;
	var xk=0;
	var yk=0;
	//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	//assuming we have already constructed correspondences from imageData as
	//described above and X and Y hold the `pattern_idx`(image1points) and
	//`screen_idx`(image2points) of the matches array for every
	//pixel respectively for each pixel in { "x": p_kp.x, "y": p_kp.y }
	//and { "x": s_kp.x, "y": s_kp.y } formats for X and Y respectively (please refer to the image above)
	//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	//find respective homographic transforms for both of the models (X and Y)
	//run ransac homographic motion model helper on motion kernel to find
	//*only* the strong inliers (good matches) excluding away all the others
	//(works on static models as well)
	//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  var mm_kernel = new jsfeat.motion_model.homography2d();
	var homo3x3 = new jsfeat.matrix_t(3, 3, jsfeat.F32C1_t);
	var ransac_param = new jsfeat.ransac_params_t(num_model_points,reproj_threshold, 0.5, 0.99);
	var match_mask = new jsfeat.matrix_t(500, 1, jsfeat.U8C1_t)
	//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	if (jsfeat.motion_estimator.ransac //runs once for static models and iterates if motion is detected
	 (ransac_param,
		mm_kernel,
		X,
		Y,
		strong_inliers,
		homo3x3,
		match_mask))
	{
     for (var i = 0; i < count; ++i) {
         if (match_mask.data[i]) {
           X[strong_inliers].x = X[i].x;
           X[strong_inliers].y = X[i].y;
           Y[strong_inliers].x = Y[i].x;
           Y[strong_inliers].y = Y[i].y;
           good_matches_locatorX[xk++]= X[i];
					 console.log(X[strong_inliers]); //==>{x: 359, y: 48},i.e., a matched point in X space
					 good_matches_locatorY[yk++]= Y[i];
					 console.log(Y[strong_inliers]); //==>{x: 65, y: 309},i.e., the corresponing point in Y space
					 strong_inliers++;
         }
     }
     mm_kernel.run(X, Y, homo3x3, strong_inliers); //run kernel directly with inliers only
  }
	var matched_points = {
		"good_matches_X": good_matches_locatorX,
		"good_matches_Y": good_matches_locatorY,
		"matches_count": strong_inliers
	};
	return matched_points; //store location of all good matches in both models (images), X and Y, as well as their count
}

function projectPointsInto(matched_points,canvas,match_thres_percentage=30,expected_good_matches=5) {
	var good_matches = matched_points.matches_count; //from above fn.
	if (num_matches) { //from `match_pattern` utility fn.
   if (good_matches >= num_matches * match_thres_percentage/100) //we have a match!
   renderer.lines(canvas, matches, num_matches);  //picks X/Y data from matches and num_matches which are GLOBALS
   if (good_matches >= expected_good_matches) //in case of a solid match when results are as we expected or even better
   renderer.box(canvas);
	 //we can stop iterations here completely for performance issues
  }
}

var renderer = { //gets hoisted
  lines: function(canvas, matches, num_matches) {
   render_matches(canvas, matches, num_matches);
  },
	box: function(canvas) {
   render_pattern_shape(canvas);
  }
}

//declarations and calls

var matcher = new ORBMatcher();

var imageA = new Image();
imageA.src = "imageA.jpg";
var imageB = new Image();
imageB.src = "imageB.jpg";

var imageDataA = matcher.findPoints(imageA);
var imageDataB = matcher.findPoints(imageB);
//construct correspondences
var matcherData = matcher.findMatchedPoints(X_,Y_);
var canvas = querySelector("#deploy-canvas").getContext("2d");
matcher.projectPointsInto(matcherData,canvas);
````
My responses for the non-code section in your comment, are as follows.

>(...)in the Leaflet environment, we'd use Leaflet polylines, most likely. And in Leaflet, we need to think in display coordinate space, not canvas coordinate space, so the question of what our "native" coordinate space is is really a question for the renderer module, not the underlying matcher library.

Definitely! I am familiar with [Leaflet's polylines](https://leafletjs.com/reference-1.4.0.html#polyline) and think that replacing `JSFeat`'s inbuilt methods with Leaflet's shouldn't be a problem at all, since we already have the indices of all the good matches, which brings me to another very important aspect of this library, the {x,y} image space conversion to `LatLng` objects. This can be achieved by a `Scale function`, that takes in the {x,y} coordinates of the matched points, and encapsulates them to <LatLng> points. What I'm proposing here is, the extensive use of the following "scaling" formula, which can be used to convert the image space points to latitudes and longitutes, hence easily being able to pinpoint the desired location on the image using the Leaflet library, and thus being able to use those coordinates in a ton of different Leaflet APIs.

**Please do let me know if you feel this formula requires any further clarification or explaination.**

![WhatsApp Image 2019-04-09 at 05 43 03](https://user-images.githubusercontent.com/33557095/55764987-4a98e980-5a8b-11e9-8f17-44d2672250d7.jpeg)

![WhatsApp Image 2019-04-09 at 05 42 43](https://user-images.githubusercontent.com/33557095/55764985-4967bc80-5a8b-11e9-9f8a-16f47f57f99f.jpeg)

I also believe that we should implement the `AS` module until after we have successfully abstracted away our polylines one, then using that build the `AS` on top of it.

>(...) that you could pass a whole `Leaflet.DistortableImageOverlay` object to, and exposing methods like `LeafletImageMatcher.matchAndAlignImageToImage(distortableImg1, distortableImg1)` (a bit wordy, but see how this would be a very Leaflet-oriented library, as opposed to the underlying Matcher library which would be usable for things apart from Leaflet).

Absolutely! I think we can actually encapsulate all the "matching functions" in a different module, while constructing the UI (`AutoStitcher`+`Lines`+`Boxes`) using a different one. This would also make the testing part more convenient, and improve the resuseability of the components as well! We can definitely brainstorm this approach in a new issue!

> what about the "confidence" of ORB match? does that mean we need the ORB match to be an object with properties, like matches = {confidence: 56, pairs: [ {x1: 252, y1: 61, x2: 116, y2: 6} ]}, for example? (or would confidence be a property of each pair?) That might mean we could return this from matcher.getMatches(), you know?

The "confidence" for each point is currently being calculated in the `match_pattern()` util. fn. and can be returned to the `getMatches` fn. created above. We might want to note that the "confidence" we are talking about here corresponds to the standards set by the `match_threshold` and ranges between 0-127. The measure of a point being above or below the `match_threshold` is currently set a property of *every* single object, so if two points were, say to be in a pair, both of their `best_dist` (confidence factors) would be the same.

> Similarly, we may want to eventually make it possible to downscale the images as an initial step where we're faced with extremely large input images.

![pyrdown](http://www.swarthmore.edu/NatSci/mzucker1/opencv-2.4.10-docs/_images/Pyramids_Tutorial_Pyramid_Theory.png)

`JSFeat` implements the `Gaussian Pyramid` method of downsampling a large image to a smaller one in it's `pyrdown` API, i.e., `jsfeat.imgproc.pyrdown(source:matrix_t, dest:matrix_t);`. Here we can set the destination matrix to as few rows and columns which will allow us to process things faster while not sacrificing the important inliers in the image, as is evident from the final (smallest) image in the image pyramid below.

![mnpyr](https://www.pyimagesearch.com/wp-content/uploads/2015/03/pyramid_example.png)

> Is there any way to randomize the order it searches, and to stop searching once it's found a certain number of high-quality matches?

Absolutely! Instead of starting from the first row and column, I believe a better approach would be starting from the means, i.e, if the image is of the resoulution 640x480, we can start our search from the `imgData`'s pixel info at row,col of [320][240] and moving out in a circular (vignette) fashion. This approach is based on the fact that majorly the images have meaningful data, or inlier clusters count maximum at the center, rather that at the borders. To stop the execution, refer to the `projectPointsInto` fn. above which we can do in case `good_matches >= expected_good_matches`, where `expected_good_matches` is a custom threshold, and can be adjusted to reduce performance issues as well!

>(...)there's an 'undo' function generated which can be bound to an Undo UI button? This could be displayed in a notice above, perhaps, where it says "Keep this stitch?" or something.

This can be easily implemented using the recent `addTool` API of the `Leafet.DistortableImage` which triggers and adds an undo button to the menu bar every time the user `stitches` two images, and is removed upon undoing the stitch action. We can display a soft popup here to notify the user of the stitch since alert would be a bit "obstructing".

[@warren](/profile/warren), no rush, and I do understand that you're really busy these days but can you please review this at the earliest so I can update my proposal accordingly and submit my final draft over to the GSoC website?

Thank you so much for reviewing this!!

[rexagod](../profile/rexagod) replying to: [GSoC proposal: Mapknitter ORB Descriptor (w/ auto-stitching, pattern training, and live video support) and LDI revamp (major UI enhancements)](../notes/rexagod/03-11-2019/gsoc-proposal-mapknitter-orb-descriptor-w-auto-stitching-pattern-training-and-live-video-support-and-ldi-revamp-major-ui-enhancements)


---
node: What are important aspects to consider about replication on the website and in the community?
author: gretchengehrke
created_at: 2016-09-29 19:09:46 +0000
timestamp: 1475176186
nid: 13500
cid: 22314
uid: 430549
---



[gretchengehrke](../profile/gretchengehrke) replying to: [What are important aspects to consider about replication on the website and in the community?](../notes/liz/09-28-2016/what-are-important-aspects-to-consider-about-replication-on-the-website-and-in-the-community)

----
To expand upon Liz's question, we’ve started talking about harnessing the unique power of open, collaborative science: the ability to massively replicate experiments and compare results, allowing us to better assess and communicate experiments’ reproducibility and resultant implications. To facilitate this more organized (and hopefully impactful engagement), we’ve started asking people to make more detailed research notes, tagged with the type of [activity](https://publiclab.org/notes/gretchengehrke/09-02-2016/activity-categories) it is, inviting people into their research to try to replicate it, so that we can all learn from the process and results. Assessing the outcomes of the replication has many factors though. Here, we’re wondering what factors are important to consider in asking facilitating replications, evaluating those replications, deciding to fork new activities, amassing information from iterated development, and anything else that seems important in terms of inviting and utilizing mass open science! 

The first thing that comes to my mind when thinking about inviting or evaluating replication is that constitutes “replication” is different for different kinds of activities: for a tool build, the verification steps can demonstrate whether or not the build replication was successful; for a lab experiment, you need to construct the same conditions and follow the same steps; for a field test, you need to follow the same steps, and you can _try_ to find similar conditions, but this is the real world, so it’s not going to be an exact replication, and it’s extremely important to document the conditions such that the way different conditions impact the results can be elucidated. So, we may need to be more clear about what exactly “replication” means for various situations. Ideas? 

Another thing that comes to my mind in terms of evaluating replications is that there is a difference between _success_ in terms of someone clearly articulating steps and someone else being able to correctly follow them, versus people _successfully_ following the same steps but finding different results (which is  _successful_ in that it can tell us something about precision or influence of various conditions, but could also be considered _failure_ in that the original data wasn’t reproducible), and _success_ in terms of being able to accomplish what you had hoped. Does anyone have ideas about how we should distinguish these? Can we come up with evaluation rubrics? Also, who should determine what is deemed successful? I’m personally prone to having the person who conducted the activity (be in initial or replication) assess its successfulness, but maybe there is a more systematic way? Or, that person assesses its successfulness using a standard rubric that we collectively create? I think that this point -- about whose duty it is to assess outcomes -- is pretty important when we consider social dynamics and democratization of science. What do people think? 
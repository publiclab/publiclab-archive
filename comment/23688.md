---
cid: 23688
node: [GSoC proposal: Mapknitter ORB Descriptor (w/ auto-stitching, pattern training, and live video support) and LDI revamp (major UI enhancements)](../notes/rexagod/03-11-2019/gsoc-proposal-mapknitter-orb-descriptor-w-auto-stitching-pattern-training-and-live-video-support-and-ldi-revamp-major-ui-enhancements)
nid: 18515
created_at: 2019-04-05 15:21:14 +0000
timestamp: 1554477674
uid: 1
author: [warren](../profile/warren)
---

OK, thanks [@rexagod](/profile/rexagod). Building on my questions in https://publiclab.org/notes/rexagod/03-11-2019/gsoc-proposal-mapknitter-orb-descriptor-w-auto-stitching-pattern-training-and-live-video-support-and-ldi-revamp-major-ui-enhancements#c22147, I think we may want to do a few things -- one, I think some of the variable names could be made more human readable -- Let's imagine our ideal abstract library we'll call `matcher` and think of what functions it will have:

```
var matcher = new OrbMatcher();

// these would output coordinates in the space of 
var points1 = matcher.findPoints(img1);
var points2 = matcher.findPoints(img2);

// this would need clear examples and docs for the format returned, and what coordinate space they're in.
// also, assuming it's stateless, would we need to pass in the original images too, so that we have extra context about the original images, or is there enough information in `points1` and `points2`?
var matches = matcher.findMatchedPoints(points1, points2);

// Alternatively, we could decide that this library should be stateful, and should store the input images:

var matcher = new StatefulMatcher();

matcher.addImages(img1, img2);

// not sure about this, but maybe:
var points0 = matcher.getPoints(0);
var points1 = matcher.getPoints(1);

// In either case, we could then use:
matcher.projectPointsInto(points0, targetCoordSpaceImg); // pass in the image whose coord space we want to use

// or, how would we do something like this?:
matcher.projectPointsIntoCanvsSpace(points0); 

// for some of the display functions the underlying library offers, like the render_pattern_matches you mentioned above, we might separate that as much as we can into a render layer of the library:
matcher.renderer.someFunctionName()

// on the other hand, if we may be displaying lots of the UI in Leaflet, we don't really want to write such specific code into the library, so maybe we'd just want to make it possible to fetch the coordinates out which allow us to use Leaflet to draw such UI elements as boxes, lines, etc?
```

Note that most of the initial methods above are in image x,y space, not canvas space; they are abstracted to be only in the coordinate spaces of the 2 input images. So, perhaps we would think of conversion to canvas space as a separate set of methods in the display code?

This essentially shifts the mental model from points in `pattern_xy` and `canvas_xy` to points in `img1Points` and `img2Points`. And the data returned via the matcher library's public API, instead of an `canvas_idx` index, would be in the coordinate space of `img1`. By default, we probably wouldn't even show the canvas onscreen. 

The idea of abstracting the UI code -- esp. if it's written in Leaflet UI code -- might need a separate interfacing library, that builds on the fundamental features in the matcher, but is called something like `LeafletImageStitcher` (riffing on the name you proposed in your code snippet above) -- that you could pass a whole `Leaflet.DistortableImageOverlay` object to, and exposing methods like `LeafletImageMatcher.matchAndAlignImageToImage(distortableImg1, distortableImg1);` (a bit wordy, but see how this would be a very Leaflet-oriented library, as opposed to the underlying Matcher library which would be usable for things apart from Leaflet).

Actually, before trying to implement an auto-stitcher, I think it's totally worthwhile to attempt the more minimal "draw lines between matched points" UI. This would be a simpler overall system, and could be built upon to achieve auto-stitching. It would also help to keep the complexity of the full auto-stitch function contained, rather than premising the design of the whole library around the final end-to-end auto-stitch. 

One example of this is that the Leaflet UI probably would not make use of the Canvas API to draw boxes, lines, etc -- Leaflet has its own polyline API. So, that's a good reason to abstract away all the render methods so that in a Canvas environment (like microscope, say) we can use those, but in the Leaflet environment, we'd use Leaflet polylines, most likely. And in Leaflet, we need to think in display coordinate space, not canvas coordinate space, so the question of what our "native" coordinate space is is really a question for the renderer module, not the underlying matcher library. What do you think of this?

Just a note -- what about the "confidence" of ORB match? does that mean we need the ORB match to be an object with properties, like `matches = {confidence: 56, pairs: [ {x1: 252, y1: 61, x2: 116, y2: 6} ]}`, for example? (or would confidence be a property of each pair?) That might mean we could return this from `matcher.getMatches()`, you know?

One more thing - thinking of speed and optimization, esp for gigantic images of 5-10mb -- in the match generator function, you note that it's brute force. Is there any way to randomize the order it searches, and to stop searching once it's found a certain number of high-quality matches? We could default to returning all matches, but being able to request just a few may be useful for some applications where speed is a factor. Similarly, we may want to eventually make it possible to downscale the images as an initial step where we're faced with extremely large input images. But we could just leave a comment inline in the code (and an open issue) to remind ourselves, as this is an optimization, not a core function, so we could think about it later. 

For the UI, what do you think about a `Stitch + undo` workflow where the lib can be made to auto-stitch one image against another, but there's an 'undo' function generated which can be bound to an Undo UI button? This could be displayed in a notice above, perhaps, where it says "Keep this stitch?" or something. 

Thanks for your detailed responses [@rexagod](/profile/rexagod). I know you have the underlying systems figured out well. Most of my input here is attempting to develop a highly readable and universal vocabulary of public methods that will make this library quick to learn, and to preserve conceptual abstractions which will allow us to use it in very flexible environments (like canvas vs. leaflet). I hope this input makes sense and apologies if I've missed a few things from your writing above... i'm trying to get through a lot of proposals this week and have to keep catching up, so please forgive me!

Again, great work, thanks!!!

[warren](../profile/warren) replying to: [GSoC proposal: Mapknitter ORB Descriptor (w/ auto-stitching, pattern training, and live video support) and LDI revamp (major UI enhancements)](../notes/rexagod/03-11-2019/gsoc-proposal-mapknitter-orb-descriptor-w-auto-stitching-pattern-training-and-live-video-support-and-ldi-revamp-major-ui-enhancements)


---
node: Comparisons: Canons, Mobiuses, 'fruit
author: donblair
created_at: 2014-04-14 19:42:57 +0000
timestamp: 1397504577
nid: 10298
cid: 8762
uid: 43651
---



[donblair](../profile/donblair) replying to: [Comparisons: Canons, Mobiuses, 'fruit](../notes/donblair/04-11-2014/comparisons-canons-mobiuses-fruit)

----
Hi Chris,

Wow, so much good info / thinking here.  Starting to dig into PeeBee's suggestions -- eager to see if I can make sense of them too -- he's very clear, but as you say, I need to wrap my head around his suggestions.

> What is a "vis-red block above lens?"

Ah -- here, I meant to refer to the Infragram filter, 'blue' or 'red-block' filter that folks have been adding on top of e.g. a modified Canon A495 with its internal NIR-block filter removed, in order to effect an 'Infragram conversion.' ...

> Why is the above different for the A495 and Mobius?

Oh crap. You're right. It shouldn't be. Lemme fix that!
 
> Does the "lens" row apply to either of the two sensors?

*That's* a good question, too.  I was wondering how to think about that.  I suppose what I'm trying to get at in this chart is how many 'manipulations' of the image data we're receiving using these various tools are a) occurring and b) within our control.  So in that sense, I guess I shouldn't really apply a 'lens' row for the 'single pixel' sensors.  I recall debating this, and then deciding that maybe I'd add it in case we might think about adding some sort of 'diffusion cap' above the sensors.  But for now, I should wipe out those rows.

> Does the "per-pixel color filter array" apply to the NIR related columns for the two sensors?

Ah, good question.  In the case of the RGB sensor, it oughtn't, right?  There's an NIR block filter above the entire sensor; but in the case of the Luminosity sensor, I forget now how that's set up. Let me find that description again ...

>  The same adjustment can be made to the rest of the image (unless you take sensor readings from multiple areas in the scene, but then why bother with the camera?).

Ah, this recalls for me some earlier discussions about the various Infragram use-cases. I've gotten the impression (perhaps wrong) that when it comes to analyzing vegetation in this manner, the changes in leaf-by-leaf lighting conditions make it hard to assess whether individual plants are doing particular well, or not.  But when one zooms out to the scale of, say, a large field of vegetation, then some of these variations average out a bit better, and you might be able to detect patterns -- e.g. that corner of the field looks like it needs more water.  If several 'spot calibrations' with a ground-based, single-pixel sensor -- making measurements on several leaves at several locations in crops growing in a certain part of the field, and averaging them -- were then used to calibrate Infragram imagery taken from a balloon- or kite-based camera, perhaps this would make for a nice, calibrated system? Similar to how (I believe?) satellite imagery is calibrated by taking spectrometer readings on the ground?

Let me quickly fix the chart, and dig into your other ideas ... 
---
node: Improvements to SpectralWorkbench.org and PublicLab.org
author: warren
created_at: 2014-04-16 14:19:40 +0000
timestamp: 1397657980
nid: 9433
cid: 8794
uid: 1
---



[warren](../profile/warren) replying to: [Improvements to SpectralWorkbench.org and PublicLab.org](../notes/warren/10-10-2013/improvements-to-spectralworkbench-org-and-publiclab-org)

----
Ah, i see what you're asking. OK:

Ideally, we would measure all wavelengths equally in each pixel -- the diffraction grating should be separating out the colors (wavelengths) and each pixel records a different one, from 0-100% of the sensor's ability to detect light. (not an absolute measurement).

However, since the camera has R, G, and B filters on each pixel, we instead get three images, each of which is filtering for a region of the spectrum. We *could* try to use data only from the "best" band -- like, measure green light generally with the green sensors, and red with the red. But what about light that's partway between green and red? (yellow) Where do we decide where to switch sensors? It probably varies per camera, too. So at this point, for consistency, we just add all three channels together. This is not as good as a monochrome (full spectrum) sensor, but those just aren't available at the low price and high resolution of webcams. 

Does that help?
---
node: NDVI microscopy
author: cfastie
created_at: 2018-07-19 03:39:40 +0000
timestamp: 1531971580
nid: 16741
cid: 20169
uid: 554
---



[cfastie](../profile/cfastie) replying to: [NDVI microscopy](../notes/MaggPi/07-18-2018/ndvi-micrsocopy)

----
MaggPi,  

This approach is quite intriguing. You are correct that NDVI was invented and then used for 40 years primarily for high altitude aerial or satellite images (each pixel can include multiple square meters or hectares of ground surface). Producing NDVI images from low altitude (kite, balloon, drone) photography requires major changes in the way the NDVI images are interpreted (each pixel can include a small part of single leaf), and this fact is lost on some practitioners. Producing NDVI images from microscopic photos of plants requires even more modifications to how we interpret the results (only the pixels capturing chloroplasts might be relevant).

One important consideration is that the microscopic photography must be done on freshly prepared plant material. Is that a cross section of a living pine needle? NDVI depends on the way plant pigments absorb visible versus NIR light and that changes as the plant is stressed. Preparing the leaf sample for microscopy can cause stress, but it could take hours or a day for that stress to change the way light is absorbed by the pigments. So there is time to capture photos while the pigments are still behaving normally.

Traditionally, photos used for NDVI are captured while vegetation is illuminated by the sun and sky. Sunlight has a particular ratio of red:NIR wavelengths which is key to the production of NDVI images. NDVI is just a measure of the difference between how much of the red versus NIR in that sunlight is reflected from the vegetation. If the vegetation were illuminated by a giant LED, the NDVI result could be drastically different because the ratio of red:NIR in the LED might differ from that of sunlight.

In your example, you used different LEDs to make the visible and NIR photos (I assume you used the red channel in the RGB photo for your red data?). So there was some ratio of red:NIR in the light effectively illuminating your subjects, but we don't know how it compares to the ratio in sunlight. 

Did you use a Pi NoIR camera to take both photos? If so, how much NIR was emitted by the white LED? And how much of that NIR was captured by the red channel in the sensor? These answers determine what wavelengths were captured by the channel you used to represent visible light.

Because you used two separate photos to make each NDVI image, the exposure of the photos could have altered the effective ratio. Healthy foliage reflects several times more NIR than red light. That is the difference that must be captured to make an NDVI image. If you make two photos of plant pigments, one of reflected red light and one of reflected NIR light, and both are well exposed photos, then there will be little difference between the brightness of the pigments in the two photos. The adjustments made to the exposure (brightness) of each photo will have made the brightness of both photos similar. Computing NDVI for each pixel with those two photos will have little meaning.

A potential workable approach could be to:

1. Illuminate your living or freshly prepared plant sample with sunlight or artificial light which mimics the red:NIR ratio of sunlight.  
2. Take a photo with a full spectrum camera (pi NoIR) with a red filter in front of the lens. That filter must transmit only red wavelengths.  
3. Take a photo of the identical scene with the same camera with an NIR filter in front of the lens. That filter must transmit only NIR light.   
4. The two filters should transmit the same percentage (e.g., 100%) of either red or NIR light.  
5. The same exposure settings must be used for both photos (same ISO, shutter speed, f/stop, gain).  
6. Use the red channel of the red photo and any channel (or the mean or sum of all three) of the NIR photo to compute NDVI (NIR-Red)/(NIR+Red) for each pixel.

A remaining problem with this approach is that the camera sensor is not as sensitive to NIR light as it is to red light. So even if you control everything as described above, the values in the NIR photo will not be as large as they should be to represent how bright the NIR light reflected from the sample was. To adjust the result to compensate for that, you could use the sum of all three channels in the NIR photo or you could just multiply the NIR value in each pixel by a fudge factor (also called calibration constant). Or you could find a red filter which transmits only a portion of the span of red wavelengths (e.g., 640-660nm) and an NIR filter which transmits a wide range of NIR wavelengths (720-900nm).

With such a system, you should be able to capture microphotographs which produce NDVI images which show clearly where chloroplasts are. Or you can take a normal color photograph in which chloroplasts will be the only thing that is green.

Chris




 


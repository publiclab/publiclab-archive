---
cid: 23866
node: ![Horticultural Spectrometer Upgrade - Planning](../notes/jenjimah/04-07-2019/horticultural-spectroscope-upgrade-planning)
nid: 18991
created_at: 2019-04-10 20:39:39 +0000
timestamp: 1554928779
uid: 54025
author: stoft
---

Ok, the project concepts are taking shape which always prompts more thoughts.

1) By 2D mapping the light spectrum and relative intensity, you will have some arrays of data characterizing the light source. That suggests two approaches: A) average the data (and possibly rotate the plants or B) keep the plant layout and apply the 2D mapping to the plant data. Either way, the plans would have be uniquely identified but with averaging, data would be tossed which generally obscures later analysis.

2) True, some light sources do not have 'steady-state' power sources (eg. HPS lamps in gymnasiums which pulse at 60 Hz and wreck havoc with normal digital photography). Others might 'flicker' at higher rates. To avoid the issue either the sampling rate must be much faster (and then process the data to get an average) or use sensor integration which is much longer than the pulse rate to let the integration time perform the averaging. Fortunately, plant growth is a very slow process relative to all of this.

2a) An 8MPix sensor has about 2800 lines and at the typical 30 fps max scan rate, lines are read at about 85kHz max. Since fluorescent lights with a ballast run at about 20kHz, the higher scan rate could probably produce 'banding'.

2b) So, what to do? I'd suggest dropping the sensor scan rate to increase integration time and then collect multiple spectrum data sets for time-dependent post-measurement analysis. Multiple spectra data sets are a good idea anyway to A) assure the data is valid and B) look for errant low-frequency noise and stability issues. 3D 'waterfall' plots can be very useful for analyzing spectral plots over time.

2c) Averaging. Caution is always warranted when averaging. It is best, when possible, to have more raw data to analyze because noise can take many forms. One large "glitch", which represent only single uncorrelated anomalies, can significantly affect an average value -- but that error will be hidden. Also, averaging always throws data away. Think more in terms of extracting useful information than just tossing information away.

Averaging true Gaussian noise is extracting the true signal from totally uncorrelated noise. However, the noise in PLab spectrometer plots is not all Gaussian so simple averaging does toss useful information which can be extracted by looking for raw data correlations over multiple spectra data sets. Again, fortunately, plants are slow critters.

3) You might look into the variance of sunlight vs a Solux source. While the Solux source does requires a few minutes warm-up, it is very stable. Yes, both  sun and Solux are stable, they do have slightly different spectra. From my own observations, I suspect that a Solux lamp (reserved only for doing 'calibrations' so as to drop lamp aging to effectively zero) would provide a more repeatable reference.

Here's the question: if, with periodic checks against the sun, there was some variation. Do you decide that variation was the Solux lamp or just atmospheric water vapor, time of year, smog, etc. -- or some of each? My thought: Use a commercial device (with known, published measurement uncertainties) to measure the Solux. Do that a number of times with repeat measurement setups (to account for those variations) and check the stability. This is not to avoid using a solar reference, but as a potential means of detecting which source is the variable in terms of calibration.

Sorry for the long post; just tossing around some ideas to consider.

Dave

